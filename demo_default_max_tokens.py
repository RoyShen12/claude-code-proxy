#!/usr/bin/env python3
"""
Demo script showing DEFAULT_MAX_TOKENS functionality.
"""

import requests
import json
import time

def demo_default_max_tokens():
    """Demonstrate the DEFAULT_MAX_TOKENS feature with real requests."""

    base_url = "http://localhost:8082"

    print("ğŸ¯ DEFAULT_MAX_TOKENS Feature Demo")
    print("=" * 50)
    print("This demo shows how the proxy handles different max_tokens values")
    print("Make sure the proxy server is running with DEFAULT_MAX_TOKENS=1024")
    print("")

    # Test cases
    test_cases = [
        {
            "name": "Normal Request (max_tokens: 512)",
            "description": "Should preserve the requested max_tokens value",
            "request": {
                "model": "claude-3-haiku",
                "messages": [{"role": "user", "content": "Say hello briefly"}],
                "max_tokens": 512
            },
            "expected_behavior": "Uses requested 512 tokens"
        },
        {
            "name": "Invalid Request (max_tokens: 0)",
            "description": "Should use DEFAULT_MAX_TOKENS instead",
            "request": {
                "model": "claude-3-haiku",
                "messages": [{"role": "user", "content": "Say hello briefly"}],
                "max_tokens": 0
            },
            "expected_behavior": "Uses DEFAULT_MAX_TOKENS (1024) instead of 0"
        },
        {
            "name": "Small Request (max_tokens: 50)",
            "description": "Should use DEFAULT_MAX_TOKENS due to MIN_TOKENS_LIMIT",
            "request": {
                "model": "claude-3-haiku",
                "messages": [{"role": "user", "content": "Say hello briefly"}],
                "max_tokens": 50
            },
            "expected_behavior": "Uses DEFAULT_MAX_TOKENS (1024) as 50 < MIN_TOKENS_LIMIT"
        },
        {
            "name": "Large Request (max_tokens: 8192)",
            "description": "Should be clamped to MAX_TOKENS_LIMIT",
            "request": {
                "model": "claude-3-haiku",
                "messages": [{"role": "user", "content": "Say hello briefly"}],
                "max_tokens": 8192
            },
            "expected_behavior": "Clamped to MAX_TOKENS_LIMIT (4096) regardless of DEFAULT_MAX_TOKENS"
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"ğŸ§ª Test {i}: {test_case['name']}")
        print(f"ğŸ“ Description: {test_case['description']}")
        print(f"ğŸ¯ Expected: {test_case['expected_behavior']}")
        print()

        print("ğŸ“¤ Request payload:")
        print(json.dumps(test_case['request'], indent=2))
        print()

        try:
            # Send request to proxy
            response = requests.post(
                f"{base_url}/v1/messages",
                json=test_case['request'],
                headers={"Content-Type": "application/json"},
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                print("âœ… Request successful!")
                print(f"ğŸ“¥ Response type: {result.get('type', 'unknown')}")
                print(f"ğŸ¯ Model used: {result.get('model', 'unknown')}")

                # Show usage if available
                if 'usage' in result:
                    print(f"ğŸ“Š Token usage: {result['usage']}")

                # Show first part of content
                if 'content' in result and result['content']:
                    content = result['content'][0].get('text', '')[:100]
                    print(f"ğŸ’¬ Response preview: {content}...")

            else:
                print(f"âŒ Request failed with status {response.status_code}")
                print(f"ğŸ” Error: {response.text}")

        except requests.exceptions.ConnectionError:
            print("âŒ Connection failed - Make sure the proxy server is running on localhost:8082")
        except Exception as e:
            print(f"âŒ Error: {e}")

        print()
        print("-" * 50)
        print()

        # Small delay between requests
        time.sleep(1)

    print("ğŸ“‹ Summary:")
    print("ğŸ”’ MAX_TOKENS_LIMIT vs ğŸ”„ DEFAULT_MAX_TOKENS:")
    print("â€¢ MAX_TOKENS_LIMIT=4096: ä¸Šé™æ§åˆ¶ - æ‰€æœ‰è¯·æ±‚éƒ½ä¸èƒ½è¶…è¿‡æ­¤å€¼")
    print("â€¢ DEFAULT_MAX_TOKENS=1024: é»˜è®¤æ›¿ä»£ - ä»…å½“è¯·æ±‚å€¼æ— æ•ˆæ—¶ä½¿ç”¨")
    print("â€¢ MIN_TOKENS_LIMIT=100: ä¸‹é™æ§åˆ¶ - ç¡®ä¿æœ€å°tokenæ•°é‡")
    print("")
    print("ğŸ’¡ å¤„ç†é€»è¾‘:")
    print("1. æ£€æŸ¥è¯·æ±‚å€¼æ˜¯å¦æœ‰æ•ˆ (>0 ä¸” >=MIN_TOKENS_LIMIT)")
    print("2. æ— æ•ˆæ—¶ç”¨DEFAULT_MAX_TOKENSæ›¿ä»£ï¼Œæœ‰æ•ˆæ—¶ä¿ç•™åŸå€¼")
    print("3. æœ€åç¡®ä¿ç»“æœåœ¨[MIN_TOKENS_LIMIT, MAX_TOKENS_LIMIT]èŒƒå›´å†…")
    print("")
    print("ğŸ¯ å®é™…åº”ç”¨:")
    print("â€¢ æ§åˆ¶æˆæœ¬: MAX_TOKENS_LIMITé˜²æ­¢è¿‡é‡æ¶ˆè€—")
    print("â€¢ é¿å…é”™è¯¯: DEFAULT_MAX_TOKENSå¤„ç†æ— æ•ˆè¯·æ±‚")
    print("â€¢ ä¿è¯è´¨é‡: MIN_TOKENS_LIMITç¡®ä¿å“åº”å®Œæ•´")

if __name__ == "__main__":
    demo_default_max_tokens()
